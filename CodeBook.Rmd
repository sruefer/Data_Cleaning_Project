---
title: "Code Book"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Raw Data Sets

The original data is made of several files, all located in the folder `UCI HAR Dataset`. The below 
files were imported into R and used for this project:

- "features.txt": List of all 561 features.
- "activity_labels.txt": Links the class labels with their activity name (6 activities).
- "train/X_train.txt": Training set, containing 7532 observations of 561 variables.
- "train/y_train.txt": Training labels.
- "test/X_test.txt": Test set, containing 2947 observations of 561 variables.
- "test/y_test.txt": Test labels.
- "train/subject_train.txt": Each row identifies the subject who performed the activity for each window sample. Its range is from 1 to 30.
- "test/subject_ttest.txt": Each row identifies the subject who performed the activity for each window sample. Its range is from 1 to 30.

## Details about the Original Data

Below information is from the README.txt file and features_info.txt file included in the folder containing the original data.


Human Activity Recognition Using Smartphones Dataset
Version 1.0

Jorge L. Reyes-Ortiz, Davide Anguita, Alessandro Ghio, Luca Oneto.
Smartlab - Non Linear Complex Systems Laboratory
DITEN - Universit√† degli Studi di Genova.
Via Opera Pia 11A, I-16145, Genoa, Italy.

The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each
person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING)
wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we
captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments
have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two
sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.

The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in
fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal,
which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body
acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a
filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating
variables from the time and frequency domain. See 'features_info.txt' for more details.

For each record it is provided:

- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.
- Triaxial Angular velocity from the gyroscope. 
- A 561-feature vector with time and frequency domain variables. 
- Its activity label. 
- An identifier of the subject who carried out the experiment.

The dataset includes the following files:

- 'README.txt'

- 'features_info.txt': Shows information about the variables used on the feature vector.

- 'features.txt': List of all features.

- 'activity_labels.txt': Links the class labels with their activity name.

- 'train/X_train.txt': Training set.

- 'train/y_train.txt': Training labels.

- 'test/X_test.txt': Test set.

- 'test/y_test.txt': Test labels.

The following files are available for the train and test data. Their descriptions are equivalent. 

- 'train/subject_train.txt': Each row identifies the subject who performed the activity for each window sample. Its range is from 1 to 30. 

- 'train/Inertial Signals/total_acc_x_train.txt': The acceleration signal from the smartphone accelerometer X axis in standard gravity units 'g'. Every row shows a 128 element vector. The same description applies for the 'total_acc_x_train.txt' and 'total_acc_z_train.txt' files for the Y and Z axis. 

- 'train/Inertial Signals/body_acc_x_train.txt': The body acceleration signal obtained by subtracting the gravity from the total acceleration. 

- 'train/Inertial Signals/body_gyro_x_train.txt': The angular velocity vector measured by the gyroscope for each window sample. The units are radians/second. 

Notes: 

- Features are normalized and bounded within [-1,1].
- Each feature vector is a row on the text file.


The features selected for this database come from the accelerometer and gyroscope 3-axial raw signals tAcc-XYZ and tGyro-XYZ. These time domain signals (prefix 't' to denote time) were captured at a constant rate of 50 Hz. Then they were filtered using a median filter and a 3rd order low pass Butterworth filter with a corner frequency of 20 Hz to remove noise. Similarly, the acceleration signal was then separated into body and gravity acceleration signals (tBodyAcc-XYZ and tGravityAcc-XYZ) using another low pass Butterworth filter with a corner frequency of 0.3 Hz. 

Subsequently, the body linear acceleration and angular velocity were derived in time to obtain Jerk signals (tBodyAccJerk-XYZ and tBodyGyroJerk-XYZ). Also the magnitude of these three-dimensional signals were calculated using the Euclidean norm (tBodyAccMag, tGravityAccMag, tBodyAccJerkMag, tBodyGyroMag, tBodyGyroJerkMag). 

Finally a Fast Fourier Transform (FFT) was applied to some of these signals producing fBodyAcc-XYZ, fBodyAccJerk-XYZ, fBodyGyro-XYZ, fBodyAccJerkMag, fBodyGyroMag, fBodyGyroJerkMag. (Note the 'f' to indicate frequency domain signals). 

These signals were used to estimate variables of the feature vector for each pattern:  
'-XYZ' is used to denote 3-axial signals in the X, Y and Z directions.

- tBodyAcc-XYZ
- tGravityAcc-XYZ
- tBodyAccJerk-XYZ
- tBodyGyro-XYZ
- tBodyGyroJerk-XYZ
- tBodyAccMag
- tGravityAccMag
- tBodyAccJerkMag
- tBodyGyroMag
- tBodyGyroJerkMag
- fBodyAcc-XYZ
- fBodyAccJerk-XYZ
- fBodyGyro-XYZ
- fBodyAccMag
- fBodyAccJerkMag
- fBodyGyroMag
- fBodyGyroJerkMag

The set of variables that were estimated from these signals are: 

- mean(): Mean value
- std(): Standard deviation
- mad(): Median absolute deviation 
- max(): Largest value in array
- min(): Smallest value in array
- sma(): Signal magnitude area
- energy(): Energy measure. Sum of the squares divided by the number of values. 
- iqr(): Interquartile range 
- entropy(): Signal entropy
- arCoeff(): Autorregresion coefficients with Burg order equal to 4
- correlation(): correlation coefficient between two signals
- maxInds(): index of the frequency component with largest magnitude
- meanFreq(): Weighted average of the frequency components to obtain a mean frequency
- skewness(): skewness of the frequency domain signal 
- kurtosis(): kurtosis of the frequency domain signal 
- bandsEnergy(): Energy of a frequency interval within the 64 bins of the FFT of each window.
- angle(): Angle between to vectors.

Additional vectors obtained by averaging the signals in a signal window sample. These are used on the angle() variable:

- gravityMean
- tBodyAccMean
- tBodyAccJerkMean
- tBodyGyroMean
- tBodyGyroJerkMean


# Loading and Transforming the Data

Below are the detailed steps for loading and transforming the datasets documented.

## 1) Merge Training and Test Sets

Prior to loading the data, the `dplyr` package was loaded for data processing (not shown in this report). 

```{r, include=FALSE}
library(dplyr)
```

Loading the meta data is straight forward with the `read.table()` function.

```{r}
setwd("~/GitHub/Data_Cleaning_Project/UCI HAR Dataset")
features <- read.table("features.txt")
activities <- read.table("activity_labels.txt")
```

A quicklook at the data sets reveals dimensions and content.

```{r}
str(features)
activities
```

As the text files do not contain headers, the feature names can be extracted and used during importing the training and test set.

```{r}
# Extract variable names
col_names <- as.character(features$V2)
head(col_names)
```

Now, training and test sets are loaded into R. Afterwards, they are merged together and a label column as well as a subject column are added from the respective files and data sets.

```{r load, cache=TRUE}
# Load Training Set
setwd("~/GitHub/Data_Cleaning_Project/UCI HAR Dataset/train")
X_train <- read.table("X_train.txt", col.names = col_names)
y_train <- read.table("y_train.txt")
subject_train <- read.table("subject_train.txt")

# Load Test Set
setwd("~/GitHub/Data_Cleaning_Project/UCI HAR Dataset/test")
X_test <- read.table("X_test.txt", col.names = col_names)
y_test <- read.table("y_test.txt")
subject_test <- read.table("subject_test.txt")

# Merge with rbind()
X.raw <- rbind(X_train, X_test)  # merge observation sets
y.raw <- rbind(y_train, y_test)  # merge label sets
subject.raw <- rbind(subject_train, subject_test)  # merge subject sets

# Add Label to merged data set
x.labelled <- X.raw
x.labelled$activity <- y.raw$V1  # Add label column called "activity"
x.labelled$subject <- subject.raw$V1  # Add subject column called "subject"
dim(x.labelled)  # print data set dimensions
```

## 2) Extract Mean and Standard Deviation from Measurements

The merged dataset contains 563 variables; however, only the mean and standard deviation measurements are required. By using regular expressions, only these as well as the label and subject column are kept and saved into a new data set.

```{r}
filtered.cols <- grep("mean\\.|std\\.|^activity|^subject", names(x.labelled))
x <- x.labelled[, filtered.cols]
dim(x)
```


## 3) Descriptive Activity Labels

The activity descriptions contained in the `activities` data set are used to change the values of the label column to descriptions.

```{r}
# Convert from factor to character
activities$V2 <- as.character(activities$V2)

# Loop through each activity
for (i in 1:nrow(activities)) {
      # Re-assign new value: instead of a number the respective activity as described.
      x$activity[x$activity == i] <- activities$V2[activities$V1 == i]
}

# Print a few rows of the last columns of x
head(x[, 65:68])

# Print unique values of activity column
unique(x$activity)
```

## 4) Label the Data Set

Because the column names were extracted already in Step (1), the variables are already labelled. However, the variable names are cleaned up by removing the periods in the names.

```{r}
# Show first 10 names
head(names(x), 10)

# Remove periods/dots
new_names <- gsub("\\.", "", names(x))
names(x) <- new_names

# Show new names (first 10)
head(names(x), 10)
```

## 5) Create Summary Data Set

A summary data set, containing the mean values of all variables for each activity and subject is created as below.

```{r}
# Use the dplyr package to create summary data set
x <- tbl_df(x)
x2 <- x %>% 
      group_by(activity, subject) %>% 
      summarise_each(funs(sum), -c(activity, subject))
# Dimensions
dim(x2)
x2
```

The summary data set contains 180 rows, resulting from each possible combination of 6 activities and 30 subjects.

Finally, the data sets are saved to CSV files.

```{r}
setwd("~/GitHub/Data_Cleaning_Project")
write.csv(x, "x.csv")
write.csv(x2, "x2.csv")
```

# Variables

## Variables of the Cleaned Data

The variables of the cleaned data set, `x`, are the same as of the original data set, but filtered so that only Mean and Standard Deviation variants are included.

Summary of variables:

- Mean values: 33 variables
- Standard Deviation values: 33 variables
- Others: 2 variables

Variables for mean values:

```{r}
grep("mean", names(x), value = TRUE)
```

Variables for standard deviation values:

```{r}
grep("std", names(x), value = TRUE)
```

Other variables: there are two additional variables, which are subject and activity label:

```{r}
grep("^activity|^subject", names(x), value = TRUE)
```

Table of Variables:

```{r}
glimpse(x)
```


## Variables of the Summarized Data

The summarized data, `x2`, contains the same variables as `x`, i.e. 68 variables in total. However, it has only 180 observations, containing the grouped mean values of each variable.

```{r}
glimpse(x2)
```

# Summary

This concludes the project assignment. All steps should be reproducible with the data provided in the repository. The script described above is available in the R file provided, named `run_Analysis.R`. 